# idetc2025-designqa

Problem Statement 2
DesignQA

    How well can LLMs understand design engineering documents?
    Will model hallucinations exacerbate design biases and lead to design catastrophes?
    Can you develop a method to beat the best-performing model on an engineering design benchmark, DesignQA?

A trusted AI assistant that can deeply understand technical engineering documents would be invaluable for engineers, especially for tasks that require frequent reference to standards or compliance checks. Unfortunately, research shows that vision-language models (VLMs) currently struggle with synthesizing technical information from documents with engineering images. To evaluate these capabilities, the DesignQA benchmark—composed of 1149 question-answer pairs based on real data from the MIT Motorsports team—tests VLMs on their ability to interpret and apply the Formula SAE rulebook. GPT-4o, Gemini 1.0, Claude Opus, and other models have been evaluated on DesignQA. However, no existing model achieves a perfect score on any DesignQA subset. The results for many of the benchmark subsets indicate that there is substantial room for VLM improvement when it comes to understanding and cross-analyzing engineering documentation and images.

